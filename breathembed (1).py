# -*- coding: utf-8 -*-
"""BreathEmbed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MOBuPHL0g6Sbifayv31bAU_DqzID8nyT
"""

import os
import glob
import random
import torch
import pandas as pd
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchaudio
import torchaudio.transforms as T
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
import soundfile as sf # Added for load_audio
import librosa
from transformers import Wav2Vec2Model
import scipy.signal # Added for band-pass filter

from google.colab import drive
drive.mount('/content/drive')

"""```mermaid
graph TD
    A[Input WAV File] --> B{load_audio, rms_normalize, pad_or_truncate}
    B --> C{Augmentor.augment_waveform (Wav1)}
    B --> D{Augmentor.augment_waveform (Wav2)}

    C --> E[torch.tensor (Wav1)]
    D --> F[torch.tensor (Wav2)]

    E --> G{torchaudio.transforms.MelSpectrogram}
    F --> H{torchaudio.transforms.MelSpectrogram}

    G --> I[Mel Spectrogram (Wav1)]
    H --> J[Mel Spectrogram (Wav2)]

    I --> K{torchaudio.functional.amplitude_to_DB}
    J --> L{torchaudio.functional.amplitude_to_DB}

    K --> M[Mel Spectrogram dB (Wav1)]
    L --> N[Mel Spectrogram dB (Wav2)]

    M --> O{Augmentor.spec_augment_mel (Optional)}
    N --> P{Augmentor.spec_augment_mel (Optional)}

    O --> Q[Final Spec1 for SpectralHead]
    P --> R[Final Spec2 for SpectralHead]
```
"""

# Execute the SSL training and checkpoint saving process
# This cell contains the code for dataset indexing, splitting, model initialization,
# training loop, and saving the trained SSL model components.

root = "/content/drive/MyDrive/ScienceFairBreathEmbed/BRACETS/Data"
entries = index_bracets_dataset(root)
train_entries, val_entries, test_entries = split_by_patient(entries)

print(f"Number of training examples: {len(train_entries)}")
print(f"Number of validation examples: {len(val_entries)}")
print(f"Number of test examples: {len(test_entries)}")

if not train_entries:
    print("No training entries found. Check dataset path.")
else:
    train_ds = BracetsMultiHeadDataset(train_entries)
    val_ds = BracetsMultiHeadDataset(val_entries)

    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=8, shuffle=False)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # Shared Backbone
    backbone = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base").to(device)

    # Heads
    temporal = TemporalHead(input_dim=768).to(device)
    spectral = SpectralHead().to(device)
    energy   = EnergyHead(input_dim=768).to(device)

    optimizer = torch.optim.Adam(
        list(backbone.parameters()) +
        list(temporal.parameters()) +
        list(spectral.parameters()) +
        list(energy.parameters()),
        lr=2e-5 # Lower LR for fine-tuning backbone
    )

    train_ssl(backbone, temporal, spectral, energy, train_loader, val_loader, optimizer, device)



# ---------------------------
# Save SSL Checkpoint
# ---------------------------
ssl_save_path = "/content/drive/MyDrive/ScienceFairBreathEmbed/ssl_checkpoints"
os.makedirs(ssl_save_path, exist_ok=True)

# Use the correct variable names (temporal, spectral, energy)
torch.save({
    "backbone_state": backbone.state_dict(),
    "temporal_state": temporal.state_dict(),
    "spectral_state": spectral.state_dict(),
    "energy_state": energy.state_dict(),
    "ssl_config": {
        "proj_dim": 256,
        "sample_rate": DEFAULT_SR
    }
}, f"{ssl_save_path}/ssl_pretrained.pt")

print("ðŸ”µ SSL pretrained representations saved!")

# Define global constants
DEFAULT_SR = 16000  # Default sample rate
DEFAULT_WINDOW_SEC = 3.0 # Default window duration in seconds

# ---------------------------
# Utilities: audio I/O & preprocessing
# ---------------------------
def load_audio(path: str, sr: int = DEFAULT_SR):
    wav, orig_sr = sf.read(path, dtype='float32')
    # Ensure mono
    if wav.ndim > 1:
    # Resample if needed
        wav = np.mean(wav, axis=1)
    if orig_sr != sr:
        # Fix for librosa >= 0.10.0 which requires keyword arguments
        wav = librosa.resample(wav, orig_sr=orig_sr, target_sr=sr)
    return wav.astype(np.float32), sr

def rms_normalize(wav: np.ndarray, target_rms: float = 0.1):
    rms = np.sqrt(np.mean(wav ** 2) + 1e-9)
    if rms > 0:
        return wav * (target_rms / rms)
    return wav

def pad_or_truncate(wav: np.ndarray, sr: int, duration: float = DEFAULT_WINDOW_SEC):
    target_len = int(sr * duration)
    if len(wav) < target_len:
        pad = target_len - len(wav)
        wav = np.concatenate([wav, np.zeros(pad, dtype=wav.dtype)])
    else:
        wav = wav[:target_len]
    return wav

def index_bracets_dataset(root_dir):
    """
    Returns a list of dict entries:
    {
        "patient_id": "01",
        "sensor": "EXTD",
        "filepath": "/path/to/audio.wav"
    }
    """
    entries = []
    root = Path(root_dir)
    patient_dirs = sorted([p for p in root.iterdir() if p.is_dir()])
    for patient_dir in patient_dirs:
        pid = patient_dir.name  # "01", "02", ...
        sound_dir = patient_dir / "Sound"

        # Three sensors
        for sensor_name in ["EXTD", "DIAP", "BELL"]:
            sensor_dir = sound_dir / sensor_name
            if not sensor_dir.exists():
                continue
            wav_files = list(sensor_dir.glob("*.wav"))
            for wf in wav_files:
                entries.append({
                    "patient_id": pid,
                    "sensor": sensor_name,
                    "filepath": str(wf)
                })
    return entries

def split_by_patient(entries, test_size=0.2, val_size=0.1, seed=42):
    patient_ids = sorted(list({e["patient_id"] for e in entries}))
    if not patient_ids:

    # Handle case with very few patients
        print("Warning: No patients found.")
        return [], [], []
    if len(patient_ids) < 2:
        return entries, [], []

    train_p, test_p = train_test_split(patient_ids, test_size=test_size, random_state=seed)
    if len(train_p) > 1:
        train_p, val_p  = train_test_split(train_p, test_size=val_size, random_state=seed)
    else:
        val_p = []

    def filter_entries(pids):
        return [e for e in entries if e["patient_id"] in pids]

    return (
        filter_entries(train_p),
        filter_entries(val_p),
        filter_entries(test_p)
    )

# ---------------------------
# Augmentations
# ---------------------------
class Augmentor:
    def __init__(self, sr=DEFAULT_SR, crop_len_s=DEFAULT_WINDOW_SEC):
        self.sr = sr
        self.crop_len = int(sr * crop_len_s)

    def random_crop(self, wav: np.ndarray):
        n = len(wav)
        if n <= self.crop_len:
            return pad_or_truncate(wav, self.sr, duration=self.crop_len / self.sr)
        start = random.randint(0, n - self.crop_len)
        return wav[start:start + self.crop_len]

    def add_noise(self, wav: np.ndarray, snr_db_min=10, snr_db_max=30):
        snr = random.uniform(snr_db_min, snr_db_max)
        rms = np.sqrt(np.mean(wav**2) + 1e-9)
        noise_rms = rms / (10 ** (snr / 20.0))
        noise = np.random.randn(len(wav)).astype(np.float32) * noise_rms
        return wav + noise

    def random_gain(self, wav: np.ndarray, db_min=-6, db_max=6):
        g = 10 ** (random.uniform(db_min, db_max) / 20.0)
        return wav * g

    def time_stretch(self, wav: np.ndarray, min_rate=0.9, max_rate=1.1):
        rate = random.uniform(min_rate, max_rate)
        stretched = librosa.effects.time_stretch(wav, sr=self.sr, rate=rate)
        return pad_or_truncate(stretched, self.sr)

    def pitch_shift(self, wav: np.ndarray, n_steps_min=-1.0, n_steps_max=1.0):
        n_steps = random.uniform(n_steps_min, n_steps_max)
        shifted = librosa.effects.pitch_shift(wav, sr=self.sr, n_steps=n_steps)
        return pad_or_truncate(shifted, self.sr)

    def random_bandpass_filter(self, wav: np.ndarray, lowcut_min=50, lowcut_max=500, highcut_min=2000, highcut_max=7000, order=4):
        # Randomly choose cutoff frequencies within a reasonable range
        lowcut = random.uniform(lowcut_min, lowcut_max)
        highcut = random.uniform(highcut_min, highcut_max)

        # Ensure highcut is greater than lowcut
        if lowcut >= highcut:
            lowcut, highcut = highcut * 0.5, lowcut * 1.2 # adjust if necessary
            if lowcut >= highcut: # Make sure they don't overlap or are same
                lowcut = min(lowcut, self.sr / 2 - 1000)
                highcut = min(highcut, self.sr / 2 - 100)
                if lowcut >= highcut:
                    return wav # Skip if still problematic

        nyquist = 0.5 * self.sr
        low = lowcut / nyquist
        high = highcut / nyquist

        if low >= 1.0 or high >= 1.0 or low <= 0.0 or high <= 0.0 or low >= high:
            # Handle invalid cutoff frequencies
            return wav

        b, a = scipy.signal.butter(order, [low, high], btype='band')
        filtered_wav = scipy.signal.lfilter(b, a, wav)
        return filtered_wav.astype(np.float32)

    def spec_augment_mel(self, mel: np.ndarray, freq_mask_param=8, time_mask_param=20):
        # mel: (n_mels, T)
        # frequency mask
        mel = mel.copy()
        f = random.randint(0, freq_mask_param)
        f0 = random.randint(0, max(0, mel.shape[0]-f))
        # time mask
        mel[f0:f0+f, :] = 0
        t = random.randint(0, time_mask_param)
        t0 = random.randint(0, max(0, mel.shape[1]-t))
        mel[:, t0:t0+t] = 0
        return mel

        # Compose several augmentations
    def augment_waveform(self, wav: np.ndarray):
        wav = self.random_crop(wav)
        if random.random() < 0.4:
            wav = self.add_noise(wav, snr_db_min=8, snr_db_max=25)
        if random.random() < 0.3:
            wav = self.random_gain(wav)
        if random.random() < 0.2:
            try:
                wav = self.time_stretch(wav)
            except Exception:
                pass
        if random.random() < 0.1:
            try:
                wav = self.pitch_shift(wav)
            except Exception:
                pass
        if random.random() < 0.2: # New augmentation: band-pass filter
            try:
                wav = self.random_bandpass_filter(wav)
            except Exception as e:
                print(f"Error applying band-pass filter: {e}")
                pass
        return wav

# -----------------------------------
# BracetsMultiHeadDataset for SSL training..Applies Augmentations and load audio
# ----------------------------------
class BracetsMultiHeadDataset(Dataset):
    def __init__(self, entries, sr=DEFAULT_SR, window_sec=DEFAULT_WINDOW_SEC, n_mels=64):
        self.entries = entries
        self.sr = sr
        self.window_sec = window_sec
        self.augmentor = Augmentor(sr=sr, crop_len_s=window_sec)
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=sr, n_fft=1024, hop_length=256, n_mels=n_mels
        )

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        entry = self.entries[idx]
        filepath = entry["filepath"]

        wav_np, _ = load_audio(filepath, sr=self.sr)
        wav_np = rms_normalize(wav_np)
        wav_np = pad_or_truncate(wav_np, self.sr, duration=self.window_sec)

        # Generate two augmented views (numpy arrays)
        wav1_np = self.augmentor.augment_waveform(wav_np)
        wav2_np = self.augmentor.augment_waveform(wav_np)

        # Convert to torch tensors
        wav1 = torch.tensor(wav1_np, dtype=torch.float32)
        wav2 = torch.tensor(wav2_np, dtype=torch.float32)

        # Compute mel spectrograms
        spec1 = self.mel_transform(wav1)
        spec2 = self.mel_transform(wav2)

        # Convert to dB scale and apply spec augmentation (randomly)
        spec1_db = torchaudio.functional.amplitude_to_DB(spec1, multiplier=10.0, amin=1e-10, db_multiplier=0.0)
        spec2_db = torchaudio.functional.amplitude_to_DB(spec2, multiplier=10.0, amin=1e-10, db_multiplier=0.0)

        # Apply SpecAugment to mel spectrograms (on numpy and convert back)
        spec1_db_np = spec1_db.numpy()
        spec2_db_np = spec2_db.numpy()
        if random.random() < 0.4: # Same probability as in LightningModule example
             spec1_db_np = self.augmentor.spec_augment_mel(spec1_db_np)
        if random.random() < 0.4:
             spec2_db_np = self.augmentor.spec_augment_mel(spec2_db_np)

        spec1 = torch.tensor(spec1_db_np, dtype=torch.float32)
        spec2 = torch.tensor(spec2_db_np, dtype=torch.float32)

        # Note: Energy manual calculation removed; EnergyHead now uses backbone features

        return {
            "wav1": wav1,
            "wav2": wav2,
            "spec1": spec1,
            "spec2": spec2,
        }

# ---------------------------
# Models
# ---------------------------
class ProjectionHead(nn.Module):
    def __init__(self, input_dim, proj_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )
    def forward(self, x):
        return nn.functional.normalize(self.net(x), dim=-1)

class TemporalHead(nn.Module):
    def __init__(self, input_dim=768, proj_dim=256):
        super().__init__()
        # Just the projection, backbone is separate now
        self.proj = ProjectionHead(input_dim, proj_dim)

    def forward(self, pooled_features):
        return self.proj(pooled_features)

class SpectralHead(nn.Module):
    def __init__(self, proj_dim=256):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.proj = ProjectionHead(64, proj_dim)

    def forward(self, spec):
        z = self.cnn(spec.unsqueeze(1)).view(spec.size(0), -1)
        return self.proj(z)

class EnergyHead(nn.Module):
    def __init__(self, input_dim=768, proj_dim=256):
        super().__init__()
        # MLP on backbone features
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, proj_dim)
        )

    def forward(self, pooled_features):
        return nn.functional.normalize(self.mlp(pooled_features), dim=-1)

def nt_xent(z1, z2, temperature=0.1):
    batch_size = z1.shape[0]
    z = torch.cat([z1, z2], dim=0)
    sim = torch.matmul(z, z.T) / temperature
    mask = (~torch.eye(2 * batch_size, dtype=bool)).to(sim.device)
    sim = sim.masked_select(mask).view(2 * batch_size, -1)

    positives = torch.sum(z1 * z2, dim=-1) / temperature
    positives = torch.cat([positives, positives], dim=0)

    loss = -torch.log(torch.exp(positives) / torch.exp(sim).sum(dim=-1))
    return loss.mean()

# ---------------------------
# Training Functions
# ---------------------------
def validate_ssl(backbone, model_t, model_s, model_e, loader, device):
    backbone.eval(); model_t.eval(); model_s.eval(); model_e.eval()
    total_loss = 0.0
    count = 0
    with torch.no_grad():
        for batch in loader:
            wav1 = batch["wav1"].to(device)
            wav2 = batch["wav2"].to(device)
            spec1 = batch["spec1"].to(device)
            spec2 = batch["spec2"].to(device)

            feat1 = backbone(wav1).last_hidden_state
            feat2 = backbone(wav2).last_hidden_state
            pool1 = feat1.mean(dim=1)
            pool2 = feat2.mean(dim=1)

            zt1 = model_t(pool1); zt2 = model_t(pool2)
            zs1 = model_s(spec1); zs2 = model_s(spec2)
            ze1 = model_e(pool1); ze2 = model_e(pool2)

            loss = nt_xent(zt1, zt2) + nt_xent(zs1, zs2) + nt_xent(ze1, ze2)
            total_loss += loss.item()
            count += 1
    return total_loss / count if count > 0 else 0.0

def train_ssl(backbone, model_t, model_s, model_e, train_loader, val_loader, optimizer, device, epochs=20):
    print(f"Starting training for {epochs} epochs...")
    for epoch in range(epochs):
        backbone.train(); model_t.train(); model_s.train(); model_e.train()
        train_loss_sum = 0.0
        count = 0

        for batch_idx, batch in enumerate(train_loader):
            wav1 = batch["wav1"].to(device)
            wav2 = batch["wav2"].to(device)
            spec1 = batch["spec1"].to(device)
            spec2 = batch["spec2"].to(device)

            # 1. Backbone forward pass (Shared)
            # Wav2Vec2 returns [Batch, Time, Feat]
            feat1 = backbone(wav1).last_hidden_state
            feat2 = backbone(wav2).last_hidden_state

            # Pool features (Mean pooling)
            pool1 = feat1.mean(dim=1)
            pool2 = feat2.mean(dim=1)

            # 2. Temporal Head (Projection on pooled features)
            zt1 = model_t(pool1)
            zt2 = model_t(pool2)

            # 3. Spectral Head (CNN on Mel Spec)
            zs1 = model_s(spec1)
            zs2 = model_s(spec2)

            # 4. Energy Head (MLP on pooled features)
            ze1 = model_e(pool1)
            ze2 = model_e(pool2)

            loss = (
                nt_xent(zt1, zt2) +
                nt_xent(zs1, zs2) +
                nt_xent(ze1, ze2)
            )

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            train_loss_sum += loss.item()
            count += 1

        avg_train_loss = train_loss_sum / count if count > 0 else 0.0
        avg_val_loss = validate_ssl(backbone, model_t, model_s, model_e, val_loader, device)

        print(f"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

# ---------------------------
# Main Execution
# ---------------------------
root = "/content/drive/MyDrive/ScienceFairBreathEmbed/BRACETS/Data"
entries = index_bracets_dataset(root)
train_entries, val_entries, test_entries = split_by_patient(entries)

print(f"Number of training examples: {len(train_entries)}")
print(f"Number of validation examples: {len(val_entries)}")
print(f"Number of test examples: {len(test_entries)}")

if not train_entries:
    print("No training entries found. Check dataset path.")
else:
    train_ds = BracetsMultiHeadDataset(train_entries)
    val_ds = BracetsMultiHeadDataset(val_entries)

    train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=8, shuffle=False)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    # Shared Backbone
    backbone = Wav2Vec2Model.from_pretrained("facebook/wav2vec2-base").to(device)

    # Heads
    temporal = TemporalHead(input_dim=768).to(device)
    spectral = SpectralHead().to(device)
    energy   = EnergyHead(input_dim=768).to(device)

    optimizer = torch.optim.Adam(
        list(backbone.parameters()) +
        list(temporal.parameters()) +
        list(spectral.parameters()) +
        list(energy.parameters()),
        lr=2e-5 # Lower LR for fine-tuning backbone
    )

    train_ssl(backbone, temporal, spectral, energy, train_loader, val_loader, optimizer, device)



# ---------------------------
# Save SSL Checkpoint
# ---------------------------
ssl_save_path = "/content/drive/MyDrive/ScienceFairBreathEmbed/ssl_checkpoints"
os.makedirs(ssl_save_path, exist_ok=True)

# Use the correct variable names (temporal, spectral, energy)
torch.save({
    "backbone_state": backbone.state_dict(),
    "temporal_state": temporal.state_dict(),
    "spectral_state": spectral.state_dict(),
    "energy_state": energy.state_dict(),
    "ssl_config": {
        "proj_dim": 256,
        "sample_rate": DEFAULT_SR
    }
}, f"{ssl_save_path}/ssl_pretrained.pt")

print("ðŸ”µ SSL pretrained representations saved!")

