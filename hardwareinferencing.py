# -*- coding: utf-8 -*-
"""HardwareInferencing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N8lXPOpO3QT_Gg5j4G8kkAdtxkC4gwYj

## Load file and convert to tensor
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install torch torchvision torchaudio pytorch-lightning transformers librosa soundfile

import os
import glob
import random
import torch
import pandas as pd
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchaudio
import torchaudio.transforms as T
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay # Added ConfusionMatrixDisplay
import matplotlib.pyplot as plt # Added matplotlib
import soundfile as sf # Added for load_audio
import librosa
from transformers import Wav2Vec2Model, Wav2Vec2Config # NEW IMPORT

#-----Helper Functions --------------
def load_audio(path: str, sr: int = 16000):
    wav, orig_sr = sf.read(path, dtype='float32')
    # Ensure mono
    if wav.ndim > 1:
    # Resample if needed
        wav = np.mean(wav, axis=1)
    if orig_sr != sr:
        # Fix for librosa >= 0.10.0 which requires keyword arguments
        wav = librosa.resample(wav, orig_sr=orig_sr, target_sr=sr)
    return wav.astype(np.float32), sr

def rms_normalize(wav: np.ndarray, target_rms: float = 0.1):
    rms = np.sqrt(np.mean(wav ** 2) + 1e-9)
    if rms > 0:
        return wav * (target_rms / rms)
    return wav

def pad_or_truncate(wav: np.ndarray, sr: int, duration: float = 3.0):
    target_len = int(sr * duration)
    if len(wav) < target_len:
        pad = target_len - len(wav)
        wav = np.concatenate([wav, np.zeros(pad, dtype=wav.dtype)])
    else:
        wav = wav[:target_len]
    return wav

sr = 16000
window_sec = 3.0
mel_transform = torchaudio.transforms.MelSpectrogram( sample_rate=sr, n_fft=1024, hop_length=256, n_mels=64)
def __getitem__():
  filepath = "/content/drive/MyDrive/ScienceFairBreathEmbed/BRACETS/Data/78/Sound/DIAP/78_12_PBL.wav"
  try:
      wav, _ = load_audio(filepath, sr=sr)
  except Exception as e:
      print(f"Error loading {filepath}: {e}")
      # Return a dummy zero tensor if file fails
      wav = np.zeros(int(sr * window_sec), dtype=np.float32)

  wav = rms_normalize(wav)
  wav = pad_or_truncate(wav, sr, duration=window_sec)
  wav_tensor = torch.tensor(wav, dtype=torch.float32)  # [L]

  # Mel for spectral head (computed on CPU here)
  mel = mel_transform(wav_tensor)
  mel_db = torchaudio.functional.amplitude_to_DB(mel, multiplier=10.0, amin=1e-10, db_multiplier=0.0)

  return wav_tensor, mel_db

"""##"""

class ProjectionHead(nn.Module):
    def __init__(self, input_dim, proj_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )
    def forward(self, x):
        return nn.functional.normalize(self.net(x), dim=-1)

class TemporalHead(nn.Module):
    def __init__(self, input_dim=768, proj_dim=256):
        super().__init__()
        # Just the projection, backbone is separate now
        self.proj = ProjectionHead(input_dim, proj_dim)

    def forward(self, pooled_features):
        return self.proj(pooled_features)

class SpectralHead(nn.Module):
    def __init__(self, proj_dim=256):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.proj = ProjectionHead(64, proj_dim)

    def forward(self, spec):
        z = self.cnn(spec.unsqueeze(1)).view(spec.size(0), -1)
        return self.proj(z)

class EnergyHead(nn.Module):
    def __init__(self, input_dim=768, proj_dim=256):
        super().__init__()
        # MLP on backbone features
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, proj_dim)
        )

    def forward(self, pooled_features):
        return nn.functional.normalize(self.mlp(pooled_features), dim=-1)

class FusionClassifier(nn.Module):
    def __init__(self, embed_dim_per_head=256, hidden=256, n_classes=3, dropout=0.3):
        super().__init__()
        in_dim = embed_dim_per_head * 3
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden, hidden),  # Added this layer
            nn.ReLU(),                  # Added this layer
            nn.Dropout(dropout),        # Added this layer
            nn.Linear(hidden, n_classes)
        )

    def forward(self, z_concat):
        return self.net(z_concat)

BACKBONE_FEAT_DIM = 768 # Common feature dimension for many SSL audio models (e.g., Wav2Vec2)
EMBED_DIM_PER_HEAD = 256 # From FusionClassifier's __init__
device = "cuda" if torch.cuda.is_available() else "cpu"

config = Wav2Vec2Config.from_pretrained("facebook/wav2vec2-base")
backbone = Wav2Vec2Model(config).to(device)
temporal = TemporalHead(input_dim=BACKBONE_FEAT_DIM, proj_dim=EMBED_DIM_PER_HEAD).to(device)
spectral = SpectralHead(proj_dim=EMBED_DIM_PER_HEAD).to(device)
energy   = EnergyHead(input_dim=BACKBONE_FEAT_DIM, proj_dim=EMBED_DIM_PER_HEAD).to(device)
# --- End Placeholder Model Definitions ---

# 5. Reload Saved Embedding Weights
from google.colab import drive
drive.mount('/content/drive')

ssl_save_path = "/content/drive/MyDrive/ScienceFairBreathEmbed/ssl_checkpoints"

# Load the state_dict for backbone, temporal, spectral, and energy heads
ckpt = torch.load(f"{ssl_save_path}/ssl_pretrained.pt", map_location=device)
classes = ['ILD', 'Healthy', 'Other' ]
backbone.load_state_dict(ckpt["backbone_state"])
temporal.load_state_dict(ckpt["temporal_state"])
spectral.load_state_dict(ckpt["spectral_state"])
energy.load_state_dict(ckpt["energy_state"])

# Instantiate the FusionClassifier and load its state_dict
classifier_state_dict = torch.load(f"{ssl_save_path}/fusion_classifier_weighted.pt", map_location=device)

# Assuming 'classes' list is already defined and 'EMBED_DIM_PER_HEAD' is 256
# Correct arguments for FusionClassifier's __init__
num_classes = len(classes) # 'classes' variable should be accessible

classifier = FusionClassifier(embed_dim_per_head=EMBED_DIM_PER_HEAD, n_classes=num_classes).to(device)
classifier.load_state_dict(classifier_state_dict)

classes = ['ILD', 'Healthy', 'Other']
wav_tensor, mel_db = __getitem__()
wav_tensor = wav_tensor.to(device).unsqueeze(0) # Add a batch dimension
mel_db = mel_db.to(device).unsqueeze(0) # Add a batch dimension for spectral head as well

# Backbone
outputs = backbone(wav_tensor)
pooled = outputs.last_hidden_state.mean(dim=1)

# Heads
z_t = temporal(pooled)
z_s = spectral(mel_db)
z_e = energy(pooled)
z_concat = torch.cat([z_t, z_s, z_e], dim=1)

# Classifier
logits = classifier(z_concat)
preds = logits.argmax(dim=1).cpu().numpy()
print(f"Prediction: {preds}")

"""#Instructions for Raspberry pi

## Setup instruction
1. Install latest version of Rpi os
2. Ensure to configure user credentials and WiFi during setup
3. Ensure to enable SSH

## Environment setup

Run following commands on the Rpi terminal
"""

pip install torch torchvision torchaudio pytorch-lightning transformers librosa soundfile pandas --break-system-packages

"""## Code setup
1. Download the required files in to a folder in Rpi
2. Create a new python file and save the following code in it
"""

#################### Imports #######################
import os
import glob
import random
import torch
import pandas as pd
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import torchaudio
import torchaudio.transforms as T
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay # Added ConfusionMatrixDisplay
import matplotlib.pyplot as plt # Added matplotlib
import soundfile as sf # Added for load_audio
import librosa
from transformers import Wav2Vec2Model, Wav2Vec2Config # NEW IMPORT


#-----Helper Functions --------------
def load_audio(path: str, sr: int = 16000):
    wav, orig_sr = sf.read(path, dtype='float32')
    # Ensure mono
    if wav.ndim > 1:
    # Resample if needed
        wav = np.mean(wav, axis=1)
    if orig_sr != sr:
        # Fix for librosa >= 0.10.0 which requires keyword arguments
        wav = librosa.resample(wav, orig_sr=orig_sr, target_sr=sr)
    return wav.astype(np.float32), sr

def rms_normalize(wav: np.ndarray, target_rms: float = 0.1):
    rms = np.sqrt(np.mean(wav ** 2) + 1e-9)
    if rms > 0:
        return wav * (target_rms / rms)
    return wav

def pad_or_truncate(wav: np.ndarray, sr: int, duration: float = 3.0):
    target_len = int(sr * duration)
    if len(wav) < target_len:
        pad = target_len - len(wav)
        wav = np.concatenate([wav, np.zeros(pad, dtype=wav.dtype)])
    else:
        wav = wav[:target_len]
    return wav

sr = 16000
window_sec = 3.0
mel_transform = torchaudio.transforms.MelSpectrogram( sample_rate=sr, n_fft=1024, hop_length=256, n_mels=64)
def __getitem__():
  filepath = "test_audios/76_05_AAR.wav"
  try:
      wav, _ = load_audio(filepath, sr=sr)
  except Exception as e:
      print(f"Error loading {filepath}: {e}")
      # Return a dummy zero tensor if file fails
      wav = np.zeros(int(sr * window_sec), dtype=np.float32)

  wav = rms_normalize(wav)
  wav = pad_or_truncate(wav, sr, duration=window_sec)
  wav_tensor = torch.tensor(wav, dtype=torch.float32)  # [L]

  # Mel for spectral head (computed on CPU here)
  mel = mel_transform(wav_tensor)
  mel_db = torchaudio.functional.amplitude_to_DB(mel, multiplier=10.0, amin=1e-10, db_multiplier=0.0)

  return wav_tensor, mel_db

class ProjectionHead(nn.Module):
    def __init__(self, input_dim, proj_dim=256):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, proj_dim),
            nn.ReLU(),
            nn.Linear(proj_dim, proj_dim)
        )
    def forward(self, x):
        return nn.functional.normalize(self.net(x), dim=-1)

class TemporalHead(nn.Module):
    def __init__(self, input_dim=768, proj_dim=256):
        super().__init__()
        # Just the projection, backbone is separate now
        self.proj = ProjectionHead(input_dim, proj_dim)

    def forward(self, pooled_features):
        return self.proj(pooled_features)

class SpectralHead(nn.Module):
    def __init__(self, proj_dim=256):
        super().__init__()
        self.cnn = nn.Sequential(
            nn.Conv2d(1, 32, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.proj = ProjectionHead(64, proj_dim)

    def forward(self, spec):
        z = self.cnn(spec.unsqueeze(1)).view(spec.size(0), -1)
        return self.proj(z)

class EnergyHead(nn.Module):
    def __init__(self, input_dim=768, proj_dim=256):
        super().__init__()
        # MLP on backbone features
        self.mlp = nn.Sequential(
            nn.Linear(input_dim, 512),
            nn.ReLU(),
            nn.Linear(512, proj_dim)
        )

    def forward(self, pooled_features):
        return nn.functional.normalize(self.mlp(pooled_features), dim=-1)

class FusionClassifier(nn.Module):
    def __init__(self, embed_dim_per_head=256, hidden=256, n_classes=3, dropout=0.3):
        super().__init__()
        in_dim = embed_dim_per_head * 3
        self.net = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden, hidden),  # Added this layer
            nn.ReLU(),                  # Added this layer
            nn.Dropout(dropout),        # Added this layer
            nn.Linear(hidden, n_classes)
        )

    def forward(self, z_concat):
        return self.net(z_concat)

BACKBONE_FEAT_DIM = 768 # Common feature dimension for many SSL audio models (e.g., Wav2Vec2)
EMBED_DIM_PER_HEAD = 256 # From FusionClassifier's __init__
device = "cuda" if torch.cuda.is_available() else "cpu"

config = Wav2Vec2Config.from_pretrained("facebook/wav2vec2-base")
backbone = Wav2Vec2Model(config).to(device)
temporal = TemporalHead(input_dim=BACKBONE_FEAT_DIM, proj_dim=EMBED_DIM_PER_HEAD).to(device)
spectral = SpectralHead(proj_dim=EMBED_DIM_PER_HEAD).to(device)
energy   = EnergyHead(input_dim=BACKBONE_FEAT_DIM, proj_dim=EMBED_DIM_PER_HEAD).to(device)
# --- End Placeholder Model Definitions ---

ssl_save_path = "checkpoints"

# Load the state_dict for backbone, temporal, spectral, and energy heads
ckpt = torch.load(f"{ssl_save_path}/ssl_pretrained.pt", map_location=device)
classes = ['ILD', 'Healthy', 'Other' ]
backbone.load_state_dict(ckpt["backbone_state"])
temporal.load_state_dict(ckpt["temporal_state"])
spectral.load_state_dict(ckpt["spectral_state"])
energy.load_state_dict(ckpt["energy_state"])

# Instantiate the FusionClassifier and load its state_dict
classifier_state_dict = torch.load(f"{ssl_save_path}/fusion_classifier_weighted.pt", map_location=device)

# Assuming 'classes' list is already defined and 'EMBED_DIM_PER_HEAD' is 256
# Correct arguments for FusionClassifier's __init__
num_classes = len(classes) # 'classes' variable should be accessible

classifier = FusionClassifier(embed_dim_per_head=EMBED_DIM_PER_HEAD, n_classes=num_classes).to(device)
classifier.load_state_dict(classifier_state_dict)

classes = ['ILD', 'Healthy', 'Other']
wav_tensor, mel_db = __getitem__()
wav_tensor = wav_tensor.to(device).unsqueeze(0) # Add a batch dimension
mel_db = mel_db.to(device).unsqueeze(0) # Add a batch dimension for spectral head as well

# Backbone
outputs = backbone(wav_tensor)
pooled = outputs.last_hidden_state.mean(dim=1)

# Heads
z_t = temporal(pooled)
z_s = spectral(mel_db)
z_e = energy(pooled)
z_concat = torch.cat([z_t, z_s, z_e], dim=1)

# Classifier
logits = classifier(z_concat)
preds = logits.argmax(dim=1).cpu().numpy()
print(f"Prediction: {classes[preds[0]]}")